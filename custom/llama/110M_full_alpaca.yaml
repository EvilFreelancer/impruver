# Training time
# - 1x RTX 4090 ~ 23Gb VRAM

output_dir: ./models/BitNet-llama_full_alpaca
train_path: ./train.BitNet-llama_alpaca.jsonl
val_path: ./val.BitNet-llama_alpaca.jsonl

datasets:
  - name: IlyaGusev/ru_turbo_alpaca
    converter: impruver.instruction_to_messages
    only_target_loss: false

model:
  class: custom.llama.LlamaForCausalLM
  #class: transformers.LlamaForCausalLM
  dtype: bf16
  config_class: custom.llama.LlamaConfig
  #config_class: transformers.LlamaConfig
  config:
    attention_bias: false
    bos_token_id: 1
    eos_token_id: 2
    hidden_act: silu
    hidden_size: 512
    initializer_range: 0.02
    intermediate_size: 2048
    max_position_embeddings: 512
    model_type: llama
    num_attention_heads: 32
    num_hidden_layers: 4
    num_key_value_heads: 32
    pretraining_tp: 1
    rms_norm_eps: 0.000001
    rope_scaling: null
    rope_theta: 10000.0
    tie_word_embeddings: false
    torch_dtype: bfloat16
    use_cache: true
    vocab_size: 128256

tokenizer:
  class: transformers.AutoTokenizer
  name: Vikhrmodels/Vikhr-Llama-3.2-1B-Instruct
  max_tokens_count: 4096
  special_tokens:
    pad_token_id: 128002
    pad_token: <|reserved_special_token_0|>
    bos_token_id: 128000
    bos_token: <|begin_of_text|>
    eos_token_id: 128001
    eos_token: <|eot_id|>
    unk_token_id: 3
    unk_token: <unk>

trainer:
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 1000
  save_steps: 1000
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 5
  logging_steps: 1
  adam_beta1: 0.9
  adam_beta2: 0.99
  learning_rate: 0.0008
  #min_lr: 1e-4
  num_train_epochs: 10
  #max_steps: 5000
  lr_scheduler_type: cosine
  warmup_steps: 100
  optim: adafactor
  metric_for_best_model: eval_loss
  load_best_model_at_end: true
  save_total_limit: 2
  seed: 42
  remove_unused_columns: false
  max_grad_norm: 1.0
  weight_decay: 0.08
