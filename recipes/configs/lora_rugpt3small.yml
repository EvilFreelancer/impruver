# Tokenizer
tokenizer:
  _component_: transformers.AutoTokenizer
  path: ai-forever/rugpt3small_based_on_gpt2

# Dataset
dataset:
  - _component_: impruver.dataset.chat_dataset
    source: IlyaGusev/saiga_scored
    split: train[:1000]
    max_tokens_count: 1024
seed: 42
shuffle: True

# Model Arguments
model:
  _component_: transformers.AutoModelForCausalLM
  path: ai-forever/rugpt3small_based_on_gpt2

# LoRA adapter training
lora:
  r: 32
  lora_alpha: 16
  lora_dropout: 0.0
  bias: "none"
  target_modules: [ "c_attn", "c_proj", "c_fc" ]
  modules_to_save: [ "lm_head" ]

trainer:
  evaluation_strategy: "steps"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 2
  optim: "paged_adamw_8bit"

# Model quantization settings
quantization:
  load_in_4bit: True
  bnb_4bit_compute_dtype: bf16

# Reduced precision
dtype: fp16

# Training env
device: "0"

# Result and Logging
output_dir: ./models/lora_rugpt3small
