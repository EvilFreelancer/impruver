model:
  _component_: transformers.AutoModelForCausalLM
  path: ai-forever/rugpt3small_based_on_gpt2
  load_in_8bit: True

tokenizer:
  _component_: transformers.AutoTokenizer
  path: ai-forever/rugpt3small_based_on_gpt2

lora:
  r: 32
  lora_alpha: 16
  lora_dropout: 0.0
  # bias: "none"
  target_modules: [ "c_attn", "c_proj", "c_fc" ]
  modules_to_save: [ "lm_head" ]
  # use_gradient_checkpointing: "unsloth"

quantization:
  load_in_4bit: True
  # bnb_4bit_use_double_quant: True
  # bnb_4bit_quant_type: "nf4"
  # bnb_4bit_compute_dtype: "fp32"

dtype: fp16
device: cpu
